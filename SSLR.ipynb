{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99488f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchmetrics.functional import accuracy\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import io_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b223692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = 'BNA'\n",
    "data_dir = 'D:/ShareFolder/BNA/Proc'\n",
    "out_dir = 'D:/ShareFolder/BNA/Result'\n",
    "\n",
    "# atlas = 'AICHA'\n",
    "# data_dir = 'D:/ShareFolder/AICHA_VolFC/Proc'\n",
    "# out_dir = 'D:/ShareFolder/AICHA_VolFC/Result'\n",
    "\n",
    "sessions = ['REST1', 'REST2']  \n",
    "\n",
    "runs = ['RL', 'LR']\n",
    "# connection_type = 'both'  # inter, intra, or both\n",
    "connection_type = 'intra'\n",
    "random_state = 144\n",
    "clf = 'SVC'\n",
    "\n",
    "info = dict()\n",
    "data = dict()\n",
    "\n",
    "session = 'REST1'\n",
    "run_ = 'LR'\n",
    "half = 'Left'\n",
    "\n",
    "info_fname = 'HCP_%s_half_brain_%s_%s.csv' % (atlas, session, run_)\n",
    "info[run_] = io_.read_table(os.path.join(data_dir, info_fname), index_col='ID')\n",
    "data[run_] = io_.load_half_brain(data_dir, atlas, session, run_, connection_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebbd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _base import _pick_half\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "x, y = _pick_half(data[run_])\n",
    "y = label_binarize(y, classes=[1, -1])\n",
    "# y = y.reshape((-1, 1))\n",
    "genders = info[run_]['gender'].values\n",
    "\n",
    "idx_male = np.where(genders==0)\n",
    "idx_female = np.where(genders==1)\n",
    "\n",
    "\n",
    "x = torch.from_numpy(x)\n",
    "x = x.float()\n",
    "y = torch.from_numpy(y)\n",
    "y = y.long()\n",
    "genders = torch.from_numpy(genders.reshape((-1, 1)))\n",
    "genders = genders.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2497af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d27385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, l1_hparam=0.0, l2_hparam=1.0,):\n",
    "        super().__init__()\n",
    "        self.l1_hparam = l1_hparam\n",
    "        self.l2_hparam = l2_hparam\n",
    "        self.linear = nn.Linear(n_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = torch.sigmoid(self.linear(x))\n",
    "        return pred\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        pred_loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        \n",
    "        # L1 regularizer\n",
    "        if self.l1_hparam > 0:\n",
    "            l1_reg = sum(param.abs().sum() for param in self.parameters())\n",
    "            loss += self.l1_hparam * l1_reg\n",
    "\n",
    "        # L2 regularizer\n",
    "        if self.l2_hparam > 0:\n",
    "            l2_reg = sum(param.pow(2).sum() for param in self.parameters())\n",
    "            loss += self.l2_hparam * l2_reg\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93000aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_hparam = 0.0\n",
    "l2_hparam = 1.0\n",
    "\n",
    "\n",
    "# L1 regularizer\n",
    "# if l1_hparam > 0:\n",
    "#     l1_reg = sum(param.abs().sum() for param in self.parameters())\n",
    "#     loss += self.hparams.l1_strength * l1_reg\n",
    "\n",
    "# # L2 regularizer\n",
    "# if l2_hparam > 0:\n",
    "#     l2_reg = sum(param.pow(2).sum() for param in self.parameters())\n",
    "#     loss += self.hparams.l2_strength * l2_reg\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2936e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.linalg import multi_dot\n",
    "\n",
    "def hsic(x, y):\n",
    "    \n",
    "    kx = torch.mm(x, x.T)\n",
    "    ky = torch.mm(y, y.T)\n",
    "    \n",
    "    n = x.shape[0]\n",
    "    ctr_mat = torch.eye(n) - torch.ones((n, n)) / n\n",
    "    \n",
    "    return torch.trace(torch.mm(torch.mm(torch.mm(kx, ctr_mat), ky), ctr_mat)) / (n ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "88c49fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = genders.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe5d56d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([470, 7503])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[idx_male].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "081c11f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a9eba12970>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5b77ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 0.9315\n",
      "Epoch [20/500], Loss: 0.8698\n",
      "Epoch [30/500], Loss: 0.8637\n",
      "Epoch [40/500], Loss: 0.8681\n",
      "Epoch [50/500], Loss: 0.8682\n",
      "Epoch [60/500], Loss: 0.8625\n",
      "Epoch [70/500], Loss: 0.8560\n",
      "Epoch [80/500], Loss: 0.8500\n",
      "Epoch [90/500], Loss: 0.8434\n",
      "Epoch [100/500], Loss: 0.8356\n",
      "Epoch [110/500], Loss: 0.8269\n",
      "Epoch [120/500], Loss: 0.8177\n",
      "Epoch [130/500], Loss: 0.8081\n",
      "Epoch [140/500], Loss: 0.7986\n",
      "Epoch [150/500], Loss: 0.7895\n",
      "Epoch [160/500], Loss: 0.7815\n",
      "Epoch [170/500], Loss: 0.7746\n",
      "Epoch [180/500], Loss: 0.7693\n",
      "Epoch [190/500], Loss: 0.7653\n",
      "Epoch [200/500], Loss: 0.7625\n",
      "Epoch [210/500], Loss: 0.7606\n",
      "Epoch [220/500], Loss: 0.7594\n",
      "Epoch [230/500], Loss: 0.7587\n",
      "Epoch [240/500], Loss: 0.7583\n",
      "Epoch [250/500], Loss: 0.7581\n",
      "Epoch [260/500], Loss: 0.7580\n",
      "Epoch [270/500], Loss: 0.7579\n",
      "Epoch [280/500], Loss: 0.7579\n",
      "Epoch [290/500], Loss: 0.7579\n",
      "Epoch [300/500], Loss: 0.7579\n",
      "Epoch [310/500], Loss: 0.7579\n",
      "Epoch [320/500], Loss: 0.7579\n",
      "Epoch [330/500], Loss: 0.7579\n",
      "Epoch [340/500], Loss: 0.7579\n",
      "Epoch [350/500], Loss: 0.7579\n",
      "Epoch [360/500], Loss: 0.7579\n",
      "Epoch [370/500], Loss: 0.7579\n",
      "Epoch [380/500], Loss: 0.7579\n",
      "Epoch [390/500], Loss: 0.7579\n",
      "Epoch [400/500], Loss: 0.7579\n",
      "Epoch [410/500], Loss: 0.7579\n",
      "Epoch [420/500], Loss: 0.7579\n",
      "Epoch [430/500], Loss: 0.7579\n",
      "Epoch [440/500], Loss: 0.7579\n",
      "Epoch [450/500], Loss: 0.7579\n",
      "Epoch [460/500], Loss: 0.7579\n",
      "Epoch [470/500], Loss: 0.7579\n",
      "Epoch [480/500], Loss: 0.7579\n",
      "Epoch [490/500], Loss: 0.7579\n",
      "Epoch [500/500], Loss: 0.7579\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = nn.Linear(x.shape[1], 2)\n",
    "\n",
    "# Loss and optimizer\n",
    "# nn.CrossEntropyLoss() computes softmax internally\n",
    "# criterion = F.cross_entropy() \n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.1)  \n",
    "lambda_ = 1.0\n",
    "\n",
    "# Train the model\n",
    "# total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "#         # Reshape images to (batch_size, input_size)\n",
    "#         images = images.reshape(-1, input_size)\n",
    "        \n",
    "    # Forward pass\n",
    "    y_pred = torch.sigmoid(model(x[idx_male][90:]))\n",
    "#     loss = F.cross_entropy(y_pred, y[idx_male].view(-1)) + nn.MSELoss(model(x)[], genders)\n",
    "#     loss = F.cross_entropy(y_pred, y[idx_male].view(-1)) + F.cross_entropy(torch.sigmoid(model(x)), genders.view(-1))\n",
    "    loss = F.cross_entropy(y_pred, y[idx_male][90:].view(-1)) + lambda_ * (1 - torch.sigmoid(hsic(model(x), genders.float())))\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.reshape(-1, input_size)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ebe0c122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9546, grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(hsic(model(x), genders.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bb640565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4775, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsic(model(x), genders.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c4e5a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4775, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsic(model(x), genders.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3ef99b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3751, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(y_pred, y[idx_male][90:].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9c68fc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4238, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(y_pred, y[idx_male].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5ec02df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2083, -0.1478],\n",
       "        [ 0.1530,  0.0075],\n",
       "        [ 0.1299, -0.0390],\n",
       "        ...,\n",
       "        [ 0.1722, -0.1745],\n",
       "        [ 0.2459, -0.0529],\n",
       "        [ 0.1677, -0.2335]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c3b09ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3641, -0.3649],\n",
       "        [-1.0748,  1.0764],\n",
       "        [ 0.7032, -0.7029],\n",
       "        ...,\n",
       "        [ 0.9392, -0.9394],\n",
       "        [ 0.1306, -0.1311],\n",
       "        [-0.7237,  0.7235]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2716b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 0.5460\n",
      "Epoch [20/500], Loss: 0.5104\n",
      "Epoch [30/500], Loss: 0.5195\n",
      "Epoch [40/500], Loss: 0.5267\n",
      "Epoch [50/500], Loss: 0.5269\n",
      "Epoch [60/500], Loss: 0.5258\n",
      "Epoch [70/500], Loss: 0.5258\n",
      "Epoch [80/500], Loss: 0.5259\n",
      "Epoch [90/500], Loss: 0.5258\n",
      "Epoch [100/500], Loss: 0.5258\n",
      "Epoch [110/500], Loss: 0.5259\n",
      "Epoch [120/500], Loss: 0.5259\n",
      "Epoch [130/500], Loss: 0.5259\n",
      "Epoch [140/500], Loss: 0.5259\n",
      "Epoch [150/500], Loss: 0.5259\n",
      "Epoch [160/500], Loss: 0.5259\n",
      "Epoch [170/500], Loss: 0.5259\n",
      "Epoch [180/500], Loss: 0.5259\n",
      "Epoch [190/500], Loss: 0.5259\n",
      "Epoch [200/500], Loss: 0.5259\n",
      "Epoch [210/500], Loss: 0.5259\n",
      "Epoch [220/500], Loss: 0.5259\n",
      "Epoch [230/500], Loss: 0.5259\n",
      "Epoch [240/500], Loss: 0.5259\n",
      "Epoch [250/500], Loss: 0.5259\n",
      "Epoch [260/500], Loss: 0.5259\n",
      "Epoch [270/500], Loss: 0.5259\n",
      "Epoch [280/500], Loss: 0.5259\n",
      "Epoch [290/500], Loss: 0.5259\n",
      "Epoch [300/500], Loss: 0.5259\n",
      "Epoch [310/500], Loss: 0.5259\n",
      "Epoch [320/500], Loss: 0.5259\n",
      "Epoch [330/500], Loss: 0.5259\n",
      "Epoch [340/500], Loss: 0.5259\n",
      "Epoch [350/500], Loss: 0.5259\n",
      "Epoch [360/500], Loss: 0.5259\n",
      "Epoch [370/500], Loss: 0.5259\n",
      "Epoch [380/500], Loss: 0.5259\n",
      "Epoch [390/500], Loss: 0.5259\n",
      "Epoch [400/500], Loss: 0.5259\n",
      "Epoch [410/500], Loss: 0.5259\n",
      "Epoch [420/500], Loss: 0.5259\n",
      "Epoch [430/500], Loss: 0.5259\n",
      "Epoch [440/500], Loss: 0.5259\n",
      "Epoch [450/500], Loss: 0.5259\n",
      "Epoch [460/500], Loss: 0.5259\n",
      "Epoch [470/500], Loss: 0.5259\n",
      "Epoch [480/500], Loss: 0.5259\n",
      "Epoch [490/500], Loss: 0.5259\n",
      "Epoch [500/500], Loss: 0.5259\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "model_lr = nn.Linear(x.shape[1], 2)\n",
    "\n",
    "# Loss and optimizer\n",
    "# nn.CrossEntropyLoss() computes softmax internally\n",
    "# criterion = F.cross_entropy() \n",
    "optimizer = torch.optim.Adam(model_lr.parameters(), lr=learning_rate, weight_decay=1.0)  \n",
    "\n",
    "# Train the model\n",
    "# total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "#         # Reshape images to (batch_size, input_size)\n",
    "#         images = images.reshape(-1, input_size)\n",
    "        \n",
    "    # Forward pass\n",
    "    y_pred = torch.sigmoid(model_lr(x[idx_male]))\n",
    "    loss = F.cross_entropy(y_pred, y[idx_male].view(-1))\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.reshape(-1, input_size)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a3a50f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f08c1a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1462e-06, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsic(model_lr(x), genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddcf9d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0006, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsic(model_lr(x), genders.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "721db4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.sigmoid(model(x[idx_male][:90]))\n",
    "_, target = torch.max(pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60ee8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.view((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1619b68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9778)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y[idx_male][:90], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c67124a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred_f = torch.max(torch.sigmoid(model(x[idx_female])), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0b5e328f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8637)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y[idx_female], pred_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceddde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - len(torch.where(torch.sum(torch.cat((target, y[idx_male][:90]), 1), 1) == 1)[0]) / 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3dbd4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_ = torch.sigmoid(model_lr(x[idx_female]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7eb96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred_lr_f = torch.max(proba_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33787f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9947)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y[idx_female], pred_lr_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ad0d433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02604132, -0.02598074],\n",
       "       [-0.00755683,  0.00788315],\n",
       "       [-0.02889398,  0.02911591],\n",
       "       ...,\n",
       "       [-0.01741947,  0.01761098],\n",
       "       [-0.01815327,  0.01827596],\n",
       "       [-0.01183975,  0.01192712]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.data.numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cfae572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr, _ = pearsonr(model.weight.data.numpy().T[:, 0], model_lr.weight.data.numpy().T[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "02f9806d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6640800030279606"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db2cd776",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = torch.sigmoid(model(x[idx_male]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c90f685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6217, 0.3782],\n",
       "        [0.7214, 0.2788],\n",
       "        [0.3048, 0.6953],\n",
       "        [0.6740, 0.3258],\n",
       "        [0.6117, 0.3883],\n",
       "        [0.8015, 0.1984],\n",
       "        [0.6720, 0.3279],\n",
       "        [0.6718, 0.3282],\n",
       "        [0.2897, 0.7101],\n",
       "        [0.3611, 0.6391],\n",
       "        [0.2868, 0.7133],\n",
       "        [0.6977, 0.3024],\n",
       "        [0.3619, 0.6382],\n",
       "        [0.6800, 0.3200],\n",
       "        [0.7298, 0.2703],\n",
       "        [0.7820, 0.2180],\n",
       "        [0.4299, 0.5703],\n",
       "        [0.2515, 0.7488],\n",
       "        [0.3636, 0.6365],\n",
       "        [0.2248, 0.7752],\n",
       "        [0.6439, 0.3561],\n",
       "        [0.5274, 0.4726],\n",
       "        [0.6315, 0.3682],\n",
       "        [0.7644, 0.2359],\n",
       "        [0.7404, 0.2598],\n",
       "        [0.6997, 0.3002],\n",
       "        [0.7808, 0.2193],\n",
       "        [0.2506, 0.7494],\n",
       "        [0.3737, 0.6266],\n",
       "        [0.2775, 0.7221],\n",
       "        [0.6261, 0.3739],\n",
       "        [0.7019, 0.2978],\n",
       "        [0.6322, 0.3677],\n",
       "        [0.3448, 0.6550],\n",
       "        [0.3789, 0.6208],\n",
       "        [0.7693, 0.2305],\n",
       "        [0.6400, 0.3597],\n",
       "        [0.7595, 0.2403],\n",
       "        [0.7183, 0.2818],\n",
       "        [0.2667, 0.7333],\n",
       "        [0.3370, 0.6627],\n",
       "        [0.3358, 0.6642],\n",
       "        [0.7443, 0.2556],\n",
       "        [0.6577, 0.3424],\n",
       "        [0.5276, 0.4724],\n",
       "        [0.6770, 0.3228],\n",
       "        [0.4284, 0.5716],\n",
       "        [0.7226, 0.2774],\n",
       "        [0.7464, 0.2535],\n",
       "        [0.6685, 0.3314],\n",
       "        [0.7225, 0.2776],\n",
       "        [0.3846, 0.6159],\n",
       "        [0.6842, 0.3158],\n",
       "        [0.3361, 0.6643],\n",
       "        [0.3754, 0.6247],\n",
       "        [0.7677, 0.2319],\n",
       "        [0.6568, 0.3428],\n",
       "        [0.3202, 0.6798],\n",
       "        [0.3860, 0.6140],\n",
       "        [0.2950, 0.7046],\n",
       "        [0.7083, 0.2917],\n",
       "        [0.7025, 0.2976],\n",
       "        [0.3262, 0.6740],\n",
       "        [0.3350, 0.6648],\n",
       "        [0.7687, 0.2311],\n",
       "        [0.7008, 0.2992],\n",
       "        [0.3719, 0.6281],\n",
       "        [0.7286, 0.2713],\n",
       "        [0.6826, 0.3173],\n",
       "        [0.7785, 0.2216],\n",
       "        [0.6582, 0.3417],\n",
       "        [0.2807, 0.7194],\n",
       "        [0.2785, 0.7216],\n",
       "        [0.6300, 0.3701],\n",
       "        [0.7156, 0.2846],\n",
       "        [0.6093, 0.3904],\n",
       "        [0.2979, 0.7023],\n",
       "        [0.4150, 0.5850],\n",
       "        [0.6733, 0.3268],\n",
       "        [0.3729, 0.6273],\n",
       "        [0.4236, 0.5762],\n",
       "        [0.3128, 0.6872],\n",
       "        [0.7137, 0.2862],\n",
       "        [0.6575, 0.3424],\n",
       "        [0.3856, 0.6143],\n",
       "        [0.6789, 0.3211],\n",
       "        [0.3057, 0.6942],\n",
       "        [0.3669, 0.6331],\n",
       "        [0.7810, 0.2190],\n",
       "        [0.3343, 0.6658],\n",
       "        [0.7087, 0.2914],\n",
       "        [0.7056, 0.2946],\n",
       "        [0.2864, 0.7136],\n",
       "        [0.7019, 0.2981],\n",
       "        [0.6807, 0.3190],\n",
       "        [0.7064, 0.2938],\n",
       "        [0.7328, 0.2673],\n",
       "        [0.2812, 0.7187],\n",
       "        [0.3061, 0.6935],\n",
       "        [0.4010, 0.5990],\n",
       "        [0.3597, 0.6404],\n",
       "        [0.3619, 0.6381],\n",
       "        [0.5792, 0.4207],\n",
       "        [0.2507, 0.7494],\n",
       "        [0.7155, 0.2846],\n",
       "        [0.2241, 0.7756],\n",
       "        [0.6614, 0.3383],\n",
       "        [0.3382, 0.6618],\n",
       "        [0.7272, 0.2724],\n",
       "        [0.6912, 0.3090],\n",
       "        [0.7116, 0.2887],\n",
       "        [0.6471, 0.3526],\n",
       "        [0.7176, 0.2824],\n",
       "        [0.3689, 0.6312],\n",
       "        [0.2833, 0.7165],\n",
       "        [0.7714, 0.2287],\n",
       "        [0.2845, 0.7155],\n",
       "        [0.3415, 0.6586],\n",
       "        [0.3104, 0.6895],\n",
       "        [0.3701, 0.6300],\n",
       "        [0.6984, 0.3014],\n",
       "        [0.3523, 0.6478],\n",
       "        [0.7825, 0.2176],\n",
       "        [0.3388, 0.6612],\n",
       "        [0.7750, 0.2249],\n",
       "        [0.2358, 0.7642],\n",
       "        [0.3640, 0.6356],\n",
       "        [0.7413, 0.2589],\n",
       "        [0.6812, 0.3190],\n",
       "        [0.6698, 0.3302],\n",
       "        [0.3675, 0.6327],\n",
       "        [0.7006, 0.2992],\n",
       "        [0.3486, 0.6513],\n",
       "        [0.2742, 0.7260],\n",
       "        [0.2984, 0.7017],\n",
       "        [0.3161, 0.6841],\n",
       "        [0.7362, 0.2642],\n",
       "        [0.4273, 0.5727],\n",
       "        [0.6857, 0.3141],\n",
       "        [0.2712, 0.7289],\n",
       "        [0.3957, 0.6044],\n",
       "        [0.6844, 0.3153],\n",
       "        [0.7830, 0.2170],\n",
       "        [0.3092, 0.6905],\n",
       "        [0.7035, 0.2963],\n",
       "        [0.2699, 0.7303],\n",
       "        [0.3040, 0.6962],\n",
       "        [0.2699, 0.7297],\n",
       "        [0.2887, 0.7113],\n",
       "        [0.7829, 0.2174],\n",
       "        [0.6105, 0.3894],\n",
       "        [0.6303, 0.3696],\n",
       "        [0.2477, 0.7525],\n",
       "        [0.6363, 0.3639],\n",
       "        [0.6544, 0.3453],\n",
       "        [0.3331, 0.6670],\n",
       "        [0.3697, 0.6305],\n",
       "        [0.7080, 0.2922],\n",
       "        [0.2632, 0.7369],\n",
       "        [0.7266, 0.2733],\n",
       "        [0.4677, 0.5322],\n",
       "        [0.6805, 0.3194],\n",
       "        [0.2569, 0.7432],\n",
       "        [0.7196, 0.2804],\n",
       "        [0.6546, 0.3454],\n",
       "        [0.3037, 0.6961],\n",
       "        [0.2260, 0.7740],\n",
       "        [0.7706, 0.2293],\n",
       "        [0.2568, 0.7433],\n",
       "        [0.3786, 0.6215],\n",
       "        [0.7421, 0.2577],\n",
       "        [0.3919, 0.6076],\n",
       "        [0.2870, 0.7132],\n",
       "        [0.3730, 0.6270],\n",
       "        [0.3192, 0.6807],\n",
       "        [0.6969, 0.3028],\n",
       "        [0.7188, 0.2809],\n",
       "        [0.6061, 0.3940],\n",
       "        [0.6513, 0.3490],\n",
       "        [0.7478, 0.2521],\n",
       "        [0.7243, 0.2756],\n",
       "        [0.3426, 0.6574],\n",
       "        [0.3636, 0.6362],\n",
       "        [0.7252, 0.2746],\n",
       "        [0.7689, 0.2311],\n",
       "        [0.7357, 0.2643],\n",
       "        [0.3197, 0.6803],\n",
       "        [0.6726, 0.3278],\n",
       "        [0.3161, 0.6841],\n",
       "        [0.3012, 0.6984],\n",
       "        [0.2621, 0.7379],\n",
       "        [0.6461, 0.3539],\n",
       "        [0.6824, 0.3176],\n",
       "        [0.3103, 0.6897],\n",
       "        [0.3255, 0.6747],\n",
       "        [0.3889, 0.6110],\n",
       "        [0.6520, 0.3480],\n",
       "        [0.3267, 0.6734],\n",
       "        [0.3113, 0.6887],\n",
       "        [0.6618, 0.3383],\n",
       "        [0.7322, 0.2678],\n",
       "        [0.3225, 0.6775],\n",
       "        [0.7010, 0.2989],\n",
       "        [0.7208, 0.2789],\n",
       "        [0.3012, 0.6987],\n",
       "        [0.6512, 0.3486],\n",
       "        [0.4048, 0.5950],\n",
       "        [0.6516, 0.3485],\n",
       "        [0.5891, 0.4109],\n",
       "        [0.7036, 0.2964],\n",
       "        [0.2600, 0.7399],\n",
       "        [0.5713, 0.4284],\n",
       "        [0.6680, 0.3321],\n",
       "        [0.4354, 0.5647],\n",
       "        [0.6547, 0.3449],\n",
       "        [0.3034, 0.6967],\n",
       "        [0.6683, 0.3320],\n",
       "        [0.7227, 0.2775],\n",
       "        [0.6860, 0.3142],\n",
       "        [0.7192, 0.2808],\n",
       "        [0.2976, 0.7024],\n",
       "        [0.3852, 0.6148],\n",
       "        [0.5684, 0.4317],\n",
       "        [0.2942, 0.7059],\n",
       "        [0.2822, 0.7177],\n",
       "        [0.4828, 0.5170],\n",
       "        [0.3365, 0.6635],\n",
       "        [0.6942, 0.3053],\n",
       "        [0.6753, 0.3247],\n",
       "        [0.3069, 0.6933],\n",
       "        [0.6470, 0.3532],\n",
       "        [0.7466, 0.2534],\n",
       "        [0.2267, 0.7733],\n",
       "        [0.3184, 0.6815],\n",
       "        [0.6661, 0.3338],\n",
       "        [0.3776, 0.6224],\n",
       "        [0.3543, 0.6455],\n",
       "        [0.7404, 0.2597],\n",
       "        [0.6711, 0.3288],\n",
       "        [0.2694, 0.7308],\n",
       "        [0.3694, 0.6306],\n",
       "        [0.6521, 0.3482],\n",
       "        [0.6158, 0.3841],\n",
       "        [0.4443, 0.5560],\n",
       "        [0.2159, 0.7843],\n",
       "        [0.7287, 0.2712],\n",
       "        [0.6632, 0.3366],\n",
       "        [0.7549, 0.2452],\n",
       "        [0.6742, 0.3256],\n",
       "        [0.3066, 0.6935],\n",
       "        [0.6861, 0.3142],\n",
       "        [0.3253, 0.6748],\n",
       "        [0.6844, 0.3156],\n",
       "        [0.3021, 0.6975],\n",
       "        [0.6388, 0.3610],\n",
       "        [0.7777, 0.2224],\n",
       "        [0.2760, 0.7241],\n",
       "        [0.3160, 0.6841],\n",
       "        [0.2866, 0.7137],\n",
       "        [0.3454, 0.6546],\n",
       "        [0.2951, 0.7047],\n",
       "        [0.6957, 0.3042],\n",
       "        [0.4280, 0.5718],\n",
       "        [0.3582, 0.6418],\n",
       "        [0.7205, 0.2796],\n",
       "        [0.3439, 0.6560],\n",
       "        [0.2538, 0.7463],\n",
       "        [0.7582, 0.2420],\n",
       "        [0.2917, 0.7083],\n",
       "        [0.7126, 0.2874],\n",
       "        [0.7268, 0.2733],\n",
       "        [0.6678, 0.3321],\n",
       "        [0.2457, 0.7541],\n",
       "        [0.4026, 0.5975],\n",
       "        [0.6308, 0.3691],\n",
       "        [0.3348, 0.6654],\n",
       "        [0.3575, 0.6430],\n",
       "        [0.6971, 0.3030],\n",
       "        [0.7214, 0.2787],\n",
       "        [0.2618, 0.7384],\n",
       "        [0.6805, 0.3197],\n",
       "        [0.6535, 0.3463],\n",
       "        [0.2986, 0.7017],\n",
       "        [0.7304, 0.2696],\n",
       "        [0.6580, 0.3421],\n",
       "        [0.3276, 0.6724],\n",
       "        [0.6313, 0.3687],\n",
       "        [0.3788, 0.6212],\n",
       "        [0.6700, 0.3297],\n",
       "        [0.3069, 0.6931],\n",
       "        [0.7750, 0.2252],\n",
       "        [0.3257, 0.6743],\n",
       "        [0.4482, 0.5517],\n",
       "        [0.7119, 0.2881],\n",
       "        [0.3421, 0.6579],\n",
       "        [0.6140, 0.3862],\n",
       "        [0.2992, 0.7009],\n",
       "        [0.6236, 0.3765],\n",
       "        [0.2269, 0.7733],\n",
       "        [0.4106, 0.5896],\n",
       "        [0.7271, 0.2729],\n",
       "        [0.6478, 0.3521],\n",
       "        [0.5493, 0.4507],\n",
       "        [0.4125, 0.5875],\n",
       "        [0.5885, 0.4114],\n",
       "        [0.2419, 0.7581],\n",
       "        [0.4058, 0.5941],\n",
       "        [0.4776, 0.5224],\n",
       "        [0.6222, 0.3778],\n",
       "        [0.2476, 0.7527],\n",
       "        [0.3382, 0.6621],\n",
       "        [0.7071, 0.2931],\n",
       "        [0.2793, 0.7207],\n",
       "        [0.7175, 0.2827],\n",
       "        [0.2184, 0.7817],\n",
       "        [0.7262, 0.2736],\n",
       "        [0.7128, 0.2873],\n",
       "        [0.3442, 0.6555],\n",
       "        [0.6677, 0.3325],\n",
       "        [0.7315, 0.2688],\n",
       "        [0.4352, 0.5647],\n",
       "        [0.3679, 0.6320],\n",
       "        [0.2671, 0.7329],\n",
       "        [0.7781, 0.2222],\n",
       "        [0.6985, 0.3016],\n",
       "        [0.7021, 0.2979],\n",
       "        [0.2852, 0.7149],\n",
       "        [0.7827, 0.2176],\n",
       "        [0.7310, 0.2692],\n",
       "        [0.6445, 0.3554],\n",
       "        [0.3764, 0.6233],\n",
       "        [0.7153, 0.2847],\n",
       "        [0.6883, 0.3115],\n",
       "        [0.6639, 0.3360],\n",
       "        [0.3922, 0.6078],\n",
       "        [0.6756, 0.3243],\n",
       "        [0.7011, 0.2988],\n",
       "        [0.2698, 0.7303],\n",
       "        [0.2874, 0.7128],\n",
       "        [0.3979, 0.6018],\n",
       "        [0.3359, 0.6640],\n",
       "        [0.3577, 0.6421],\n",
       "        [0.2668, 0.7333],\n",
       "        [0.4174, 0.5825],\n",
       "        [0.5089, 0.4911],\n",
       "        [0.7081, 0.2917],\n",
       "        [0.6723, 0.3277],\n",
       "        [0.6297, 0.3702],\n",
       "        [0.3050, 0.6949],\n",
       "        [0.7724, 0.2276],\n",
       "        [0.7127, 0.2872],\n",
       "        [0.4298, 0.5703],\n",
       "        [0.3992, 0.6006],\n",
       "        [0.7176, 0.2825],\n",
       "        [0.7567, 0.2432],\n",
       "        [0.7829, 0.2172],\n",
       "        [0.2972, 0.7028],\n",
       "        [0.7869, 0.2131],\n",
       "        [0.7393, 0.2609],\n",
       "        [0.7470, 0.2530],\n",
       "        [0.7146, 0.2858],\n",
       "        [0.3362, 0.6639],\n",
       "        [0.7515, 0.2486],\n",
       "        [0.7611, 0.2389],\n",
       "        [0.6358, 0.3643],\n",
       "        [0.7913, 0.2086],\n",
       "        [0.3589, 0.6411],\n",
       "        [0.6510, 0.3492],\n",
       "        [0.4018, 0.5981],\n",
       "        [0.7952, 0.2051],\n",
       "        [0.3345, 0.6655],\n",
       "        [0.7174, 0.2825],\n",
       "        [0.2874, 0.7126],\n",
       "        [0.6819, 0.3179],\n",
       "        [0.3522, 0.6478],\n",
       "        [0.7517, 0.2481],\n",
       "        [0.7245, 0.2752],\n",
       "        [0.2551, 0.7448],\n",
       "        [0.3293, 0.6707],\n",
       "        [0.7018, 0.2981],\n",
       "        [0.6535, 0.3464],\n",
       "        [0.7232, 0.2766],\n",
       "        [0.3211, 0.6789],\n",
       "        [0.2851, 0.7150],\n",
       "        [0.8100, 0.1900],\n",
       "        [0.7484, 0.2519],\n",
       "        [0.3260, 0.6740],\n",
       "        [0.6991, 0.3010],\n",
       "        [0.2978, 0.7022],\n",
       "        [0.3599, 0.6401],\n",
       "        [0.7121, 0.2883],\n",
       "        [0.2301, 0.7702],\n",
       "        [0.6885, 0.3114],\n",
       "        [0.6679, 0.3322],\n",
       "        [0.6824, 0.3174],\n",
       "        [0.7649, 0.2350],\n",
       "        [0.2448, 0.7551],\n",
       "        [0.7927, 0.2074],\n",
       "        [0.6665, 0.3336],\n",
       "        [0.6685, 0.3314],\n",
       "        [0.7478, 0.2521],\n",
       "        [0.6795, 0.3207],\n",
       "        [0.7553, 0.2447],\n",
       "        [0.2562, 0.7436],\n",
       "        [0.7226, 0.2774],\n",
       "        [0.2860, 0.7141],\n",
       "        [0.3005, 0.6995],\n",
       "        [0.6853, 0.3148],\n",
       "        [0.6871, 0.3126],\n",
       "        [0.6759, 0.3242],\n",
       "        [0.2868, 0.7133],\n",
       "        [0.6745, 0.3257],\n",
       "        [0.2866, 0.7134],\n",
       "        [0.3658, 0.6340],\n",
       "        [0.7542, 0.2454],\n",
       "        [0.3920, 0.6079],\n",
       "        [0.3508, 0.6489],\n",
       "        [0.7032, 0.2964],\n",
       "        [0.3381, 0.6622],\n",
       "        [0.2983, 0.7017],\n",
       "        [0.2806, 0.7195],\n",
       "        [0.4091, 0.5906],\n",
       "        [0.4267, 0.5733],\n",
       "        [0.3187, 0.6815],\n",
       "        [0.6580, 0.3418],\n",
       "        [0.4133, 0.5865],\n",
       "        [0.3680, 0.6319],\n",
       "        [0.3542, 0.6458],\n",
       "        [0.6387, 0.3615],\n",
       "        [0.3799, 0.6202],\n",
       "        [0.3242, 0.6758],\n",
       "        [0.3345, 0.6654],\n",
       "        [0.6997, 0.3002],\n",
       "        [0.3788, 0.6214],\n",
       "        [0.6679, 0.3319],\n",
       "        [0.3618, 0.6385],\n",
       "        [0.7135, 0.2865],\n",
       "        [0.7145, 0.2856],\n",
       "        [0.6949, 0.3051],\n",
       "        [0.6829, 0.3171],\n",
       "        [0.7586, 0.2415],\n",
       "        [0.7115, 0.2883],\n",
       "        [0.2252, 0.7746],\n",
       "        [0.3279, 0.6722],\n",
       "        [0.6852, 0.3150],\n",
       "        [0.3976, 0.6022],\n",
       "        [0.7525, 0.2476],\n",
       "        [0.2431, 0.7569],\n",
       "        [0.6760, 0.3240],\n",
       "        [0.3238, 0.6763],\n",
       "        [0.3500, 0.6504],\n",
       "        [0.7052, 0.2948],\n",
       "        [0.2709, 0.7291],\n",
       "        [0.7782, 0.2220],\n",
       "        [0.3001, 0.6995],\n",
       "        [0.2935, 0.7065],\n",
       "        [0.7633, 0.2366],\n",
       "        [0.2410, 0.7587],\n",
       "        [0.6465, 0.3536],\n",
       "        [0.6928, 0.3070],\n",
       "        [0.7060, 0.2938],\n",
       "        [0.2456, 0.7544],\n",
       "        [0.7358, 0.2643],\n",
       "        [0.6832, 0.3167],\n",
       "        [0.2572, 0.7427],\n",
       "        [0.7050, 0.2951],\n",
       "        [0.2840, 0.7159],\n",
       "        [0.7297, 0.2703],\n",
       "        [0.2270, 0.7729],\n",
       "        [0.7758, 0.2241]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1a07334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[idx_male]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a86ce94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5300],\n",
       "        [0.5439],\n",
       "        [0.5853],\n",
       "        [0.4924],\n",
       "        [0.5430],\n",
       "        [0.5362],\n",
       "        [0.5408],\n",
       "        [0.5753],\n",
       "        [0.5792],\n",
       "        [0.4828],\n",
       "        [0.5085],\n",
       "        [0.5401],\n",
       "        [0.4979],\n",
       "        [0.5711],\n",
       "        [0.5524],\n",
       "        [0.5532],\n",
       "        [0.5536],\n",
       "        [0.5169],\n",
       "        [0.6081],\n",
       "        [0.5709],\n",
       "        [0.5895],\n",
       "        [0.5334],\n",
       "        [0.5439],\n",
       "        [0.5212],\n",
       "        [0.5585],\n",
       "        [0.5460],\n",
       "        [0.4947],\n",
       "        [0.5542],\n",
       "        [0.5713],\n",
       "        [0.5619],\n",
       "        [0.5690],\n",
       "        [0.4965],\n",
       "        [0.5337],\n",
       "        [0.5985],\n",
       "        [0.5848],\n",
       "        [0.5838],\n",
       "        [0.5340],\n",
       "        [0.5559],\n",
       "        [0.5586],\n",
       "        [0.5580],\n",
       "        [0.5892],\n",
       "        [0.5250],\n",
       "        [0.5427],\n",
       "        [0.4965],\n",
       "        [0.5170],\n",
       "        [0.5473],\n",
       "        [0.5616],\n",
       "        [0.5489],\n",
       "        [0.5698],\n",
       "        [0.5459],\n",
       "        [0.5272],\n",
       "        [0.4493],\n",
       "        [0.5380],\n",
       "        [0.5603],\n",
       "        [0.5402],\n",
       "        [0.5781],\n",
       "        [0.5330],\n",
       "        [0.5279],\n",
       "        [0.5434],\n",
       "        [0.5398],\n",
       "        [0.5538],\n",
       "        [0.5610],\n",
       "        [0.5161],\n",
       "        [0.5974],\n",
       "        [0.5655],\n",
       "        [0.5515],\n",
       "        [0.5018],\n",
       "        [0.5157],\n",
       "        [0.5213],\n",
       "        [0.5257],\n",
       "        [0.5732],\n",
       "        [0.5585],\n",
       "        [0.5532],\n",
       "        [0.5828],\n",
       "        [0.5899],\n",
       "        [0.5229],\n",
       "        [0.5945],\n",
       "        [0.5194],\n",
       "        [0.5533],\n",
       "        [0.5221],\n",
       "        [0.5004],\n",
       "        [0.5747],\n",
       "        [0.5672],\n",
       "        [0.5114],\n",
       "        [0.5717],\n",
       "        [0.5207],\n",
       "        [0.5453],\n",
       "        [0.5070],\n",
       "        [0.5658],\n",
       "        [0.5205],\n",
       "        [0.4963],\n",
       "        [0.5703],\n",
       "        [0.6016],\n",
       "        [0.5104],\n",
       "        [0.5530],\n",
       "        [0.5341],\n",
       "        [0.6033],\n",
       "        [0.5336],\n",
       "        [0.5711],\n",
       "        [0.5530],\n",
       "        [0.6110],\n",
       "        [0.5833],\n",
       "        [0.5357],\n",
       "        [0.5398],\n",
       "        [0.5734],\n",
       "        [0.5757],\n",
       "        [0.5412],\n",
       "        [0.5806],\n",
       "        [0.5888],\n",
       "        [0.5936],\n",
       "        [0.5481],\n",
       "        [0.5215],\n",
       "        [0.5489],\n",
       "        [0.5570],\n",
       "        [0.5480],\n",
       "        [0.5357],\n",
       "        [0.5824],\n",
       "        [0.5263],\n",
       "        [0.5551],\n",
       "        [0.5469],\n",
       "        [0.5029],\n",
       "        [0.5400],\n",
       "        [0.5440],\n",
       "        [0.5216],\n",
       "        [0.5303],\n",
       "        [0.5794],\n",
       "        [0.5780],\n",
       "        [0.5474],\n",
       "        [0.5497],\n",
       "        [0.5653],\n",
       "        [0.5580],\n",
       "        [0.5822],\n",
       "        [0.5482],\n",
       "        [0.5849],\n",
       "        [0.5463],\n",
       "        [0.5651],\n",
       "        [0.5395],\n",
       "        [0.5277],\n",
       "        [0.4887],\n",
       "        [0.5114],\n",
       "        [0.5458],\n",
       "        [0.5247],\n",
       "        [0.5113],\n",
       "        [0.5774],\n",
       "        [0.5411],\n",
       "        [0.5580],\n",
       "        [0.5526],\n",
       "        [0.5313],\n",
       "        [0.5409],\n",
       "        [0.5422],\n",
       "        [0.5246],\n",
       "        [0.5516],\n",
       "        [0.5424],\n",
       "        [0.5583],\n",
       "        [0.5685],\n",
       "        [0.5959],\n",
       "        [0.5241],\n",
       "        [0.5275],\n",
       "        [0.5508],\n",
       "        [0.5334],\n",
       "        [0.5615],\n",
       "        [0.5099],\n",
       "        [0.5283],\n",
       "        [0.5344],\n",
       "        [0.5264],\n",
       "        [0.5274],\n",
       "        [0.5624],\n",
       "        [0.5391],\n",
       "        [0.5675],\n",
       "        [0.5567],\n",
       "        [0.5647],\n",
       "        [0.5877],\n",
       "        [0.5261],\n",
       "        [0.5311],\n",
       "        [0.5969],\n",
       "        [0.5421],\n",
       "        [0.5493],\n",
       "        [0.5389],\n",
       "        [0.5795],\n",
       "        [0.5497],\n",
       "        [0.5571],\n",
       "        [0.5148],\n",
       "        [0.5455],\n",
       "        [0.5673],\n",
       "        [0.5929],\n",
       "        [0.4972],\n",
       "        [0.5737],\n",
       "        [0.5058],\n",
       "        [0.5396],\n",
       "        [0.5287],\n",
       "        [0.5563],\n",
       "        [0.6072],\n",
       "        [0.5808],\n",
       "        [0.5451],\n",
       "        [0.5595],\n",
       "        [0.5315],\n",
       "        [0.5358],\n",
       "        [0.5832],\n",
       "        [0.5649],\n",
       "        [0.5244],\n",
       "        [0.5609],\n",
       "        [0.5460],\n",
       "        [0.6285],\n",
       "        [0.5701],\n",
       "        [0.5233],\n",
       "        [0.5314],\n",
       "        [0.5265],\n",
       "        [0.5386],\n",
       "        [0.5731],\n",
       "        [0.5084],\n",
       "        [0.5087],\n",
       "        [0.5447],\n",
       "        [0.5438],\n",
       "        [0.5965],\n",
       "        [0.5744],\n",
       "        [0.5474],\n",
       "        [0.5335],\n",
       "        [0.5029],\n",
       "        [0.5393],\n",
       "        [0.5346],\n",
       "        [0.4999],\n",
       "        [0.5152],\n",
       "        [0.5597],\n",
       "        [0.5508],\n",
       "        [0.5550],\n",
       "        [0.5459],\n",
       "        [0.5629],\n",
       "        [0.5417],\n",
       "        [0.5959],\n",
       "        [0.5815],\n",
       "        [0.5779],\n",
       "        [0.5420],\n",
       "        [0.5808],\n",
       "        [0.5468],\n",
       "        [0.5638],\n",
       "        [0.5431],\n",
       "        [0.5088],\n",
       "        [0.4964],\n",
       "        [0.5544],\n",
       "        [0.5450],\n",
       "        [0.5208],\n",
       "        [0.5291],\n",
       "        [0.5068],\n",
       "        [0.4968],\n",
       "        [0.5159],\n",
       "        [0.5281],\n",
       "        [0.5506],\n",
       "        [0.6061],\n",
       "        [0.5098],\n",
       "        [0.5848],\n",
       "        [0.5900],\n",
       "        [0.5889],\n",
       "        [0.5464],\n",
       "        [0.5497],\n",
       "        [0.5189],\n",
       "        [0.5619],\n",
       "        [0.5955],\n",
       "        [0.5354],\n",
       "        [0.5738],\n",
       "        [0.5922],\n",
       "        [0.6255],\n",
       "        [0.5371],\n",
       "        [0.5591],\n",
       "        [0.5801],\n",
       "        [0.5325],\n",
       "        [0.5700],\n",
       "        [0.5486],\n",
       "        [0.5183],\n",
       "        [0.5628],\n",
       "        [0.5389],\n",
       "        [0.4704],\n",
       "        [0.4940],\n",
       "        [0.5371],\n",
       "        [0.5414],\n",
       "        [0.5407],\n",
       "        [0.5143],\n",
       "        [0.5750],\n",
       "        [0.5168],\n",
       "        [0.5857],\n",
       "        [0.5438],\n",
       "        [0.5704],\n",
       "        [0.5066],\n",
       "        [0.5554],\n",
       "        [0.5231],\n",
       "        [0.5680],\n",
       "        [0.5694],\n",
       "        [0.5657],\n",
       "        [0.5278],\n",
       "        [0.5440],\n",
       "        [0.5603],\n",
       "        [0.5796],\n",
       "        [0.5185],\n",
       "        [0.4967],\n",
       "        [0.5566],\n",
       "        [0.5528],\n",
       "        [0.6053],\n",
       "        [0.5634],\n",
       "        [0.5251],\n",
       "        [0.5739],\n",
       "        [0.5501],\n",
       "        [0.4873],\n",
       "        [0.5472],\n",
       "        [0.5434],\n",
       "        [0.5076],\n",
       "        [0.5529],\n",
       "        [0.5646],\n",
       "        [0.5632],\n",
       "        [0.5057],\n",
       "        [0.5639],\n",
       "        [0.6002],\n",
       "        [0.5521],\n",
       "        [0.5373],\n",
       "        [0.5347],\n",
       "        [0.5537],\n",
       "        [0.5141],\n",
       "        [0.5348],\n",
       "        [0.5188],\n",
       "        [0.5477],\n",
       "        [0.5339],\n",
       "        [0.5517],\n",
       "        [0.5529],\n",
       "        [0.5431],\n",
       "        [0.5899],\n",
       "        [0.5922],\n",
       "        [0.5290],\n",
       "        [0.5207],\n",
       "        [0.5696],\n",
       "        [0.5194],\n",
       "        [0.5779],\n",
       "        [0.5079],\n",
       "        [0.5733],\n",
       "        [0.5052],\n",
       "        [0.5580],\n",
       "        [0.5605],\n",
       "        [0.4845],\n",
       "        [0.6089],\n",
       "        [0.5313],\n",
       "        [0.5818],\n",
       "        [0.5819],\n",
       "        [0.5585],\n",
       "        [0.5325],\n",
       "        [0.5333],\n",
       "        [0.5442],\n",
       "        [0.5406],\n",
       "        [0.5502],\n",
       "        [0.5777],\n",
       "        [0.5986],\n",
       "        [0.5175],\n",
       "        [0.5785],\n",
       "        [0.5745],\n",
       "        [0.5398],\n",
       "        [0.5610],\n",
       "        [0.5006],\n",
       "        [0.5396],\n",
       "        [0.5644],\n",
       "        [0.5499],\n",
       "        [0.5374],\n",
       "        [0.5125],\n",
       "        [0.5327],\n",
       "        [0.5283],\n",
       "        [0.5296],\n",
       "        [0.5431],\n",
       "        [0.5976],\n",
       "        [0.5449],\n",
       "        [0.5475],\n",
       "        [0.5502],\n",
       "        [0.5435],\n",
       "        [0.5748],\n",
       "        [0.5295],\n",
       "        [0.5914],\n",
       "        [0.5110],\n",
       "        [0.5517],\n",
       "        [0.5497],\n",
       "        [0.5458],\n",
       "        [0.5521],\n",
       "        [0.5391],\n",
       "        [0.5650],\n",
       "        [0.6187],\n",
       "        [0.5345],\n",
       "        [0.5598],\n",
       "        [0.5780],\n",
       "        [0.4962],\n",
       "        [0.5351],\n",
       "        [0.5571],\n",
       "        [0.5656],\n",
       "        [0.5490],\n",
       "        [0.5396],\n",
       "        [0.5132],\n",
       "        [0.5140],\n",
       "        [0.5749],\n",
       "        [0.5312],\n",
       "        [0.5279],\n",
       "        [0.5696],\n",
       "        [0.5423],\n",
       "        [0.5247],\n",
       "        [0.5573],\n",
       "        [0.5134],\n",
       "        [0.4987],\n",
       "        [0.5045],\n",
       "        [0.5371],\n",
       "        [0.6135],\n",
       "        [0.5414],\n",
       "        [0.5580],\n",
       "        [0.5442],\n",
       "        [0.5495],\n",
       "        [0.5389],\n",
       "        [0.5086],\n",
       "        [0.5585],\n",
       "        [0.5864],\n",
       "        [0.5412],\n",
       "        [0.5628],\n",
       "        [0.4759],\n",
       "        [0.5381],\n",
       "        [0.5669],\n",
       "        [0.5375],\n",
       "        [0.5256],\n",
       "        [0.5278],\n",
       "        [0.5763],\n",
       "        [0.5784],\n",
       "        [0.5489],\n",
       "        [0.5454],\n",
       "        [0.5545],\n",
       "        [0.5642],\n",
       "        [0.5426],\n",
       "        [0.5362],\n",
       "        [0.5500],\n",
       "        [0.5807],\n",
       "        [0.5199],\n",
       "        [0.5678],\n",
       "        [0.5053],\n",
       "        [0.5338],\n",
       "        [0.5472],\n",
       "        [0.5629],\n",
       "        [0.5515],\n",
       "        [0.5447],\n",
       "        [0.5463],\n",
       "        [0.5038],\n",
       "        [0.4959],\n",
       "        [0.4997],\n",
       "        [0.5540],\n",
       "        [0.4821],\n",
       "        [0.5464],\n",
       "        [0.5643],\n",
       "        [0.5391],\n",
       "        [0.5139],\n",
       "        [0.5528],\n",
       "        [0.5644],\n",
       "        [0.5639],\n",
       "        [0.5736],\n",
       "        [0.5510],\n",
       "        [0.5821],\n",
       "        [0.5672],\n",
       "        [0.5776],\n",
       "        [0.5635],\n",
       "        [0.5448],\n",
       "        [0.5899],\n",
       "        [0.5854],\n",
       "        [0.6237],\n",
       "        [0.5379],\n",
       "        [0.5449],\n",
       "        [0.5645],\n",
       "        [0.5530],\n",
       "        [0.5090],\n",
       "        [0.5046],\n",
       "        [0.5315],\n",
       "        [0.5425],\n",
       "        [0.5449],\n",
       "        [0.5476],\n",
       "        [0.5530],\n",
       "        [0.5372]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb0ce039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([470])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[idx_male].view(-1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
